{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#/usr/bin/python2\n",
    "'''\n",
    "By kyubyong park. kbpark.linguist@gmail.com. \n",
    "https://www.github.com/kyubyong/dc_tts\n",
    "'''\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from hyperparams import Hyperparams as hp\n",
    "from modules import *\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from data_load import get_batch, load_vocab\n",
    "from networks import TextEnc, AudioEnc, AudioDec, Attention, SSRN\n",
    "from utils import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(inputs, vocab_size, num_units, zero_pad=True, scope=\"embedding\", reuse=None):\n",
    "    '''Embeds a given tensor. \n",
    "    \n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
    "         to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
    "        should be constant zeros.\n",
    "      scope: Optional scope for `variable_scope`.  \n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
    "        should be `num_units`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table', \n",
    "                                       dtype=tf.float32, \n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]), \n",
    "                                      lookup_table[1:, :]), 0)\n",
    "\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputs,\n",
    "              scope=\"normalize\",\n",
    "              reuse=None):\n",
    "    '''Applies layer normalization that normalizes along the last axis.\n",
    "\n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`. The normalization is over the last dimension.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    outputs = tf.contrib.layers.layer_norm(inputs,\n",
    "                                           begin_norm_axis=-1,\n",
    "                                           scope=scope,\n",
    "                                           reuse=reuse)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highwaynet(inputs, num_units=None, scope=\"highwaynet\", reuse=None):\n",
    "    '''Highway networks, see https://arxiv.org/abs/1505.00387\n",
    "\n",
    "    Args:\n",
    "      inputs: A 3D tensor of shape [N, T, W].\n",
    "      num_units: An int or `None`. Specifies the number of units in the highway layer\n",
    "             or uses the input size if `None`.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "      A 3D tensor of shape [N, T, W].\n",
    "    '''\n",
    "    if not num_units:\n",
    "        num_units = inputs.get_shape()[-1]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        H = tf.layers.dense(inputs, units=num_units, activation=tf.nn.relu, name=\"dense1\")\n",
    "        T = tf.layers.dense(inputs, units=num_units, activation=tf.nn.sigmoid,\n",
    "                            bias_initializer=tf.constant_initializer(-1.0), name=\"dense2\")\n",
    "        outputs = H * T + inputs * (1. - T)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(inputs,\n",
    "           filters=None,\n",
    "           size=1,\n",
    "           rate=1,\n",
    "           padding=\"SAME\",\n",
    "           dropout_rate=0,\n",
    "           use_bias=True,\n",
    "           activation_fn=None,\n",
    "           training=True,\n",
    "           scope=\"conv1d\",\n",
    "           reuse=None):\n",
    "    '''\n",
    "    Args:\n",
    "      inputs: A 3-D tensor with shape of [batch, time, depth].\n",
    "      filters: An int. Number of outputs (=activation maps)\n",
    "      size: An int. Filter size.\n",
    "      rate: An int. Dilation rate.\n",
    "      padding: Either `same` or `valid` or `causal` (case-insensitive).\n",
    "      dropout_rate: A float of [0, 1].\n",
    "      use_bias: A boolean.\n",
    "      activation_fn: A string.\n",
    "      training: A boolean. If True, dropout is applied.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "      A masked tensor of the same shape and dtypes as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope):\n",
    "        if padding.lower() == \"causal\":\n",
    "            # pre-padding for causality\n",
    "            pad_len = (size - 1) * rate  # padding size\n",
    "            inputs = tf.pad(inputs, [[0, 0], [pad_len, 0], [0, 0]])\n",
    "            padding = \"valid\"\n",
    "\n",
    "        if filters is None:\n",
    "            filters = inputs.get_shape().as_list()[-1]\n",
    "\n",
    "        params = {\"inputs\": inputs, \"filters\": filters, \"kernel_size\": size,\n",
    "                  \"dilation_rate\": rate, \"padding\": padding, \"use_bias\": use_bias,\n",
    "                  \"kernel_initializer\": tf.contrib.layers.variance_scaling_initializer(), \"reuse\": reuse}\n",
    "\n",
    "        tensor = tf.layers.conv1d(**params)\n",
    "        tensor = normalize(tensor)\n",
    "        if activation_fn is not None:\n",
    "            tensor = activation_fn(tensor)\n",
    "\n",
    "        tensor = tf.layers.dropout(tensor, rate=dropout_rate, training=training)\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hc(inputs,\n",
    "       filters=None,\n",
    "       size=1,\n",
    "       rate=1,\n",
    "       padding=\"SAME\",\n",
    "       dropout_rate=0,\n",
    "       use_bias=True,\n",
    "       activation_fn=None,\n",
    "       training=True,\n",
    "       scope=\"hc\",\n",
    "       reuse=None):\n",
    "    '''\n",
    "    Args:\n",
    "      inputs: A 3-D tensor with shape of [batch, time, depth].\n",
    "      filters: An int. Number of outputs (=activation maps)\n",
    "      size: An int. Filter size.\n",
    "      rate: An int. Dilation rate.\n",
    "      padding: Either `same` or `valid` or `causal` (case-insensitive).\n",
    "      use_bias: A boolean.\n",
    "      activation_fn: A string.\n",
    "      training: A boolean. If True, dropout is applied.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "      A masked tensor of the same shape and dtypes as `inputs`.\n",
    "    '''\n",
    "    _inputs = inputs\n",
    "    with tf.variable_scope(scope):\n",
    "        if padding.lower() == \"causal\":\n",
    "            # pre-padding for causality\n",
    "            pad_len = (size - 1) * rate  # padding size\n",
    "            inputs = tf.pad(inputs, [[0, 0], [pad_len, 0], [0, 0]])\n",
    "            padding = \"valid\"\n",
    "\n",
    "        if filters is None:\n",
    "            filters = inputs.get_shape().as_list()[-1]\n",
    "\n",
    "\n",
    "        params = {\"inputs\": inputs, \"filters\": 2*filters, \"kernel_size\": size,\n",
    "                  \"dilation_rate\": rate, \"padding\": padding, \"use_bias\": use_bias,\n",
    "                  \"kernel_initializer\": tf.contrib.layers.variance_scaling_initializer(), \"reuse\": reuse}\n",
    "\n",
    "        tensor = tf.layers.conv1d(**params)\n",
    "        H1, H2 = tf.split(tensor, 2, axis=-1)\n",
    "        H1 = normalize(H1, scope=\"H1\")\n",
    "        H2 = normalize(H2, scope=\"H2\")\n",
    "        H1 = tf.nn.sigmoid(H1, \"gate\")\n",
    "        H2 = activation_fn(H2, \"info\") if activation_fn is not None else H2\n",
    "        tensor = H1*H2 + (1.-H1)*_inputs\n",
    "\n",
    "        tensor = tf.layers.dropout(tensor, rate=dropout_rate, training=training)\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_transpose(inputs,\n",
    "                     filters=None,\n",
    "                     size=3,\n",
    "                     stride=2,\n",
    "                     padding='same',\n",
    "                     dropout_rate=0,\n",
    "                     use_bias=True,\n",
    "                     activation=None,\n",
    "                     training=True,\n",
    "                     scope=\"conv1d_transpose\",\n",
    "                     reuse=None):\n",
    "    '''\n",
    "        Args:\n",
    "          inputs: A 3-D tensor with shape of [batch, time, depth].\n",
    "          filters: An int. Number of outputs (=activation maps)\n",
    "          size: An int. Filter size.\n",
    "          rate: An int. Dilation rate.\n",
    "          padding: Either `same` or `valid` or `causal` (case-insensitive).\n",
    "          dropout_rate: A float of [0, 1].\n",
    "          use_bias: A boolean.\n",
    "          activation_fn: A string.\n",
    "          training: A boolean. If True, dropout is applied.\n",
    "          scope: Optional scope for `variable_scope`.\n",
    "          reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "            by the same name.\n",
    "\n",
    "        Returns:\n",
    "          A tensor of the shape with [batch, time*2, depth].\n",
    "        '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        if filters is None:\n",
    "            filters = inputs.get_shape().as_list()[-1]\n",
    "        inputs = tf.expand_dims(inputs, 1)\n",
    "        tensor = tf.layers.conv2d_transpose(inputs,\n",
    "                                   filters=filters,\n",
    "                                   kernel_size=(1, size),\n",
    "                                   strides=(1, stride),\n",
    "                                   padding=padding,\n",
    "                                   activation=None,\n",
    "                                   kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                                   use_bias=use_bias)\n",
    "        tensor = tf.squeeze(tensor, 1)\n",
    "        tensor = normalize(tensor)\n",
    "        if activation is not None:\n",
    "            tensor = activation(tensor)\n",
    "\n",
    "        tensor = tf.layers.dropout(tensor, rate=dropout_rate, training=training)\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextEnc(L, training=True):\n",
    "    '''\n",
    "    Args:\n",
    "      L: Text inputs. (B, N)\n",
    "\n",
    "    Return:\n",
    "        K: Keys. (B, N, d)\n",
    "        V: Values. (B, N, d)\n",
    "    '''\n",
    "    i = 1\n",
    "    tensor = embed(L,\n",
    "                   vocab_size=len(hp.vocab),\n",
    "                   num_units=hp.e,\n",
    "                   scope=\"embed_{}\".format(i)); i += 1\n",
    "    tensor = conv1d(tensor,\n",
    "                    filters=2*hp.d,\n",
    "                    size=1,\n",
    "                    rate=1,\n",
    "                    dropout_rate=hp.dropout_rate,\n",
    "                    activation_fn=tf.nn.relu,\n",
    "                    training=training,\n",
    "                    scope=\"C_{}\".format(i)); i += 1\n",
    "    tensor = conv1d(tensor,\n",
    "                    size=1,\n",
    "                    rate=1,\n",
    "                    dropout_rate=hp.dropout_rate,\n",
    "                    training=training,\n",
    "                    scope=\"C_{}\".format(i)); i += 1\n",
    "\n",
    "    for _ in range(2):\n",
    "        for j in range(4):\n",
    "            tensor = hc(tensor,\n",
    "                            size=3,\n",
    "                            rate=3**j,\n",
    "                            dropout_rate=hp.dropout_rate,\n",
    "                            activation_fn=None,\n",
    "                            training=training,\n",
    "                            scope=\"HC_{}\".format(i)); i += 1\n",
    "    for _ in range(2):\n",
    "        tensor = hc(tensor,\n",
    "                        size=3,\n",
    "                        rate=1,\n",
    "                        dropout_rate=hp.dropout_rate,\n",
    "                        activation_fn=None,\n",
    "                        training=training,\n",
    "                        scope=\"HC_{}\".format(i)); i += 1\n",
    "\n",
    "    for _ in range(2):\n",
    "        tensor = hc(tensor,\n",
    "                        size=1,\n",
    "                        rate=1,\n",
    "                        dropout_rate=hp.dropout_rate,\n",
    "                        activation_fn=None,\n",
    "                        training=training,\n",
    "                        scope=\"HC_{}\".format(i)); i += 1\n",
    "\n",
    "    K, V = tf.split(tensor, 2, -1)\n",
    "    return K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AudioEnc(S, training=True):\n",
    "    '''\n",
    "    Args:\n",
    "      S: melspectrogram. (B, T/r, n_mels)\n",
    "\n",
    "    Returns\n",
    "      Q: Queries. (B, T/r, d)\n",
    "    '''\n",
    "    i = 1\n",
    "    tensor = conv1d(S,\n",
    "                    filters=hp.d,\n",
    "                    size=1,\n",
    "                    rate=1,\n",
    "                    padding=\"CAUSAL\",\n",
    "                    dropout_rate=hp.dropout_rate,\n",
    "                    activation_fn=tf.nn.relu,\n",
    "                    training=training,\n",
    "                    scope=\"C_{}\".format(i)); i += 1\n",
    "    tensor = conv1d(tensor,\n",
    "                    size=1,\n",
    "                    rate=1,\n",
    "                    padding=\"CAUSAL\",\n",
    "                    dropout_rate=hp.dropout_rate,\n",
    "                    activation_fn=tf.nn.relu,\n",
    "                    training=training,\n",
    "                    scope=\"C_{}\".format(i)); i += 1\n",
    "    tensor = conv1d(tensor,\n",
    "                    size=1,\n",
    "                    rate=1,\n",
    "                    padding=\"CAUSAL\",\n",
    "                    dropout_rate=hp.dropout_rate,\n",
    "                    training=training,\n",
    "                    scope=\"C_{}\".format(i)); i += 1\n",
    "    for _ in range(2):\n",
    "        for j in range(4):\n",
    "            tensor = hc(tensor,\n",
    "                            size=3,\n",
    "                            rate=3**j,\n",
    "                            padding=\"CAUSAL\",\n",
    "                            dropout_rate=hp.dropout_rate,\n",
    "                            training=training,\n",
    "                            scope=\"HC_{}\".format(i)); i += 1\n",
    "    for _ in range(2):\n",
    "        tensor = hc(tensor,\n",
    "                        size=3,\n",
    "                        rate=3,\n",
    "                        padding=\"CAUSAL\",\n",
    "                        dropout_rate=hp.dropout_rate,\n",
    "                        training=training,\n",
    "                        scope=\"HC_{}\".format(i)); i += 1\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention(Q, K, V, mononotic_attention=False, prev_max_attentions=None):\n",
    "    '''\n",
    "    Args:\n",
    "      Q: Queries. (B, T/r, d)\n",
    "      K: Keys. (B, N, d)\n",
    "      V: Values. (B, N, d)\n",
    "      mononotic_attention: A boolean. At training, it is False.\n",
    "      prev_max_attentions: (B,). At training, it is set to None.\n",
    "\n",
    "    Returns:\n",
    "      R: [Context Vectors; Q]. (B, T/r, 2d)\n",
    "      alignments: (B, N, T/r)\n",
    "      max_attentions: (B, T/r)\n",
    "    '''\n",
    "    A = tf.matmul(Q, K, transpose_b=True) * tf.rsqrt(tf.to_float(hp.d))\n",
    "    if mononotic_attention:  # for inference\n",
    "        key_masks = tf.sequence_mask(prev_max_attentions, hp.max_N)\n",
    "        reverse_masks = tf.sequence_mask(hp.max_N - hp.attention_win_size - prev_max_attentions, hp.max_N)[:, ::-1]\n",
    "        masks = tf.logical_or(key_masks, reverse_masks)\n",
    "        masks = tf.tile(tf.expand_dims(masks, 1), [1, hp.max_T, 1])\n",
    "        paddings = tf.ones_like(A) * (-2 ** 32 + 1)  # (B, T/r, N)\n",
    "        A = tf.where(tf.equal(masks, False), A, paddings)\n",
    "    A = tf.nn.softmax(A) # (B, T/r, N)\n",
    "    max_attentions = tf.argmax(A, -1)  # (B, T/r)\n",
    "    R = tf.matmul(A, V)\n",
    "    R = tf.concat((R, Q), -1)\n",
    "\n",
    "    alignments = tf.transpose(A, [0, 2, 1]) # (B, N, T/r)\n",
    "\n",
    "    return R, alignments, max_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AudioDec(R, training=True):\n",
    "    '''\n",
    "    Args:\n",
    "      R: [Context Vectors; Q]. (B, T/r, 2d)\n",
    "\n",
    "    Returns:\n",
    "      Y: Melspectrogram predictions. (B, T/r, n_mels)\n",
    "    '''\n",
    "\n",
    "    i = 1\n",
    "    tensor = conv1d(R,\n",
    "                    filters=hp.d,\n",
    "                    size=1,\n",
    "                    rate=1,\n",
    "                    padding=\"CAUSAL\",\n",
    "                    dropout_rate=hp.dropout_rate,\n",
    "                    training=training,\n",
    "                    scope=\"C_{}\".format(i)); i += 1\n",
    "    for j in range(4):\n",
    "        tensor = hc(tensor,\n",
    "                        size=3,\n",
    "                        rate=3**j,\n",
    "                        padding=\"CAUSAL\",\n",
    "                        dropout_rate=hp.dropout_rate,\n",
    "                        training=training,\n",
    "                        scope=\"HC_{}\".format(i)); i += 1\n",
    "\n",
    "    for _ in range(2):\n",
    "        tensor = hc(tensor,\n",
    "                        size=3,\n",
    "                        rate=1,\n",
    "                        padding=\"CAUSAL\",\n",
    "                        dropout_rate=hp.dropout_rate,\n",
    "                        training=training,\n",
    "                        scope=\"HC_{}\".format(i)); i += 1\n",
    "    for _ in range(3):\n",
    "        tensor = conv1d(tensor,\n",
    "                        size=1,\n",
    "                        rate=1,\n",
    "                        padding=\"CAUSAL\",\n",
    "                        dropout_rate=hp.dropout_rate,\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        training=training,\n",
    "                        scope=\"C_{}\".format(i)); i += 1\n",
    "    # mel_hats\n",
    "    logits = conv1d(tensor,\n",
    "                    filters=hp.n_mels,\n",
    "                    size=1,\n",
    "                    rate=1,\n",
    "                    padding=\"CAUSAL\",\n",
    "                    dropout_rate=hp.dropout_rate,\n",
    "                    training=training,\n",
    "                    scope=\"C_{}\".format(i)); i += 1\n",
    "    Y = tf.nn.sigmoid(logits) # mel_hats\n",
    "\n",
    "    return logits, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSRN(Y, training=True):\n",
    "    '''\n",
    "    Args:\n",
    "      Y: Melspectrogram Predictions. (B, T/r, n_mels)\n",
    "\n",
    "    Returns:\n",
    "      Z: Spectrogram Predictions. (B, T, 1+n_fft/2)\n",
    "    '''\n",
    "\n",
    "    i = 1 # number of layers\n",
    "\n",
    "    # -> (B, T/r, c)\n",
    "    tensor = conv1d(Y,\n",
    "                    filters=hp.c,\n",
    "                    size=1,\n",
    "                    rate=1,\n",
    "                    dropout_rate=hp.dropout_rate,\n",
    "                    training=training,\n",
    "                    scope=\"C_{}\".format(i)); i += 1\n",
    "    for j in range(2):\n",
    "        tensor = hc(tensor,\n",
    "                      size=3,\n",
    "                      rate=3**j,\n",
    "                      dropout_rate=hp.dropout_rate,\n",
    "                      training=training,\n",
    "                      scope=\"HC_{}\".format(i)); i += 1\n",
    "    for _ in range(2):\n",
    "        # -> (B, T/2, c) -> (B, T, c)\n",
    "        tensor = conv1d_transpose(tensor,\n",
    "                                  scope=\"D_{}\".format(i),\n",
    "                                  dropout_rate=hp.dropout_rate,\n",
    "                                  training=training,); i += 1\n",
    "        for j in range(2):\n",
    "            tensor = hc(tensor,\n",
    "                            size=3,\n",
    "                            rate=3**j,\n",
    "                            dropout_rate=hp.dropout_rate,\n",
    "                            training=training,\n",
    "                            scope=\"HC_{}\".format(i)); i += 1\n",
    "    # -> (B, T, 2*c)\n",
    "    tensor = conv1d(tensor,\n",
    "                    filters=2*hp.c,\n",
    "                    size=1,\n",
    "                    rate=1,\n",
    "                    dropout_rate=hp.dropout_rate,\n",
    "                    training=training,\n",
    "                    scope=\"C_{}\".format(i)); i += 1\n",
    "    for _ in range(2):\n",
    "        tensor = hc(tensor,\n",
    "                        size=3,\n",
    "                        rate=1,\n",
    "                        dropout_rate=hp.dropout_rate,\n",
    "                        training=training,\n",
    "                        scope=\"HC_{}\".format(i)); i += 1\n",
    "    # -> (B, T, 1+n_fft/2)\n",
    "    tensor = conv1d(tensor,\n",
    "                    filters=1+hp.n_fft//2,\n",
    "                    size=1,\n",
    "                    rate=1,\n",
    "                    dropout_rate=hp.dropout_rate,\n",
    "                    training=training,\n",
    "                    scope=\"C_{}\".format(i)); i += 1\n",
    "\n",
    "    for _ in range(2):\n",
    "        tensor = conv1d(tensor,\n",
    "                        size=1,\n",
    "                        rate=1,\n",
    "                        dropout_rate=hp.dropout_rate,\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        training=training,\n",
    "                        scope=\"C_{}\".format(i)); i += 1\n",
    "    logits = conv1d(tensor,\n",
    "               size=1,\n",
    "               rate=1,\n",
    "               dropout_rate=hp.dropout_rate,\n",
    "               training=training,\n",
    "               scope=\"C_{}\".format(i))\n",
    "    Z = tf.nn.sigmoid(logits)\n",
    "    return logits, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx, idx2char = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'slice_input_producer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e8187cf9863b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Jupyter\\Notebooks\\Text_to_speech\\DC-TTS\\dc_tts-master\\data_load.py\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# Create Queues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mfpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_input_producer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfpaths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m# Parse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'slice_input_producer'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
